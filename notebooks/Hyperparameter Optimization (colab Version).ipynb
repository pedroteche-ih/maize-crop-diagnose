{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CiF48ejUuMWt",
        "outputId": "764abc0b-f056-428b-d9a3-dddb709aa6a3"
      },
      "outputs": [],
      "source": [
        "!pip install optuna, python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8HDpQNvruIJl",
        "outputId": "b94debc0-9e1d-4473-a4b8-64d4bf840123"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os, optuna\n",
        "import warnings\n",
        "\n",
        "from tensorflow import keras\n",
        "from google.colab import drive\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(dotenv_path=\"local_paths.env\")\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "K = keras.backend\n",
        "# Changing default dir\n",
        "\n",
        "# optuna.logging.set_verbosity(optuna.logging.WARNING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "l3TOZDJt_yO_",
        "outputId": "1a31b0a4-1607-4bbd-a3cd-57a291dc44f8"
      },
      "outputs": [],
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != \"/device:GPU:0\":\n",
        "    raise SystemError(\"GPU device not found\")\n",
        "print(\"Found GPU at: {}\".format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKYq7XM_Eg9M"
      },
      "outputs": [],
      "source": [
        "!mkdir 'data'\n",
        "!cp -r {os.getenv(\"GOOGLE_DRIVE_PATH\")} 'data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7PoyM1UwfYT"
      },
      "outputs": [],
      "source": [
        "# %cd '/content/drive/MyDrive/maize-crop-diagnose/data'\n",
        "# !unzip train.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfDxBn2_uIJn"
      },
      "outputs": [],
      "source": [
        "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        iterations,\n",
        "        max_lr=1e-3,\n",
        "        start_lr=None,\n",
        "        start_mom=0.95,\n",
        "        min_mom=0.85,\n",
        "        last_iterations=None,\n",
        "        last_lr=None,\n",
        "    ):\n",
        "        self.iterations = iterations\n",
        "        self.max_lr = max_lr\n",
        "        self.start_lr = start_lr or max_lr / 10\n",
        "        self.start_mom = start_mom\n",
        "        self.min_mom = min_mom\n",
        "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
        "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
        "        self.last_lr = last_lr or self.start_lr / 1000\n",
        "        self.iteration = 0\n",
        "\n",
        "    def _interpolate(self, iter1, iter2, lr1, lr2):\n",
        "        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1\n",
        "\n",
        "    def on_batch_begin(self, batch, logs):\n",
        "        if self.iteration < self.half_iteration:\n",
        "            lr = self._interpolate(0, self.half_iteration, self.start_lr, self.max_lr)\n",
        "            mom = self._interpolate(\n",
        "                0, self.half_iteration, self.start_mom, self.min_mom\n",
        "            )\n",
        "        elif self.iteration < 2 * self.half_iteration:\n",
        "            lr = self._interpolate(\n",
        "                self.half_iteration, 2 * self.half_iteration, self.max_lr, self.start_lr\n",
        "            )\n",
        "            mom = self._interpolate(\n",
        "                self.half_iteration,\n",
        "                2 * self.half_iteration,\n",
        "                self.min_mom,\n",
        "                self.start_mom,\n",
        "            )\n",
        "        else:\n",
        "            lr = self._interpolate(\n",
        "                2 * self.half_iteration, self.iterations, self.start_lr, self.last_lr\n",
        "            )\n",
        "            mom = self.start_mom\n",
        "        self.iteration += 1\n",
        "        K.set_value(self.model.optimizer.learning_rate, lr)\n",
        "        K.set_value(self.model.optimizer.momentum, mom)\n",
        "\n",
        "\n",
        "class OneCycleSchedulerNoMom(tf.keras.callbacks.Callback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        iterations,\n",
        "        max_lr=1e-3,\n",
        "        start_lr=None,\n",
        "        last_iterations=None,\n",
        "        last_lr=None,\n",
        "    ):\n",
        "        self.iterations = iterations\n",
        "        self.max_lr = max_lr\n",
        "        self.start_lr = start_lr or max_lr / 10\n",
        "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
        "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
        "        self.last_lr = last_lr or self.start_lr / 1000\n",
        "        self.iteration = 0\n",
        "\n",
        "    def _interpolate(self, iter1, iter2, lr1, lr2):\n",
        "        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1\n",
        "\n",
        "    def on_batch_begin(self, batch, logs):\n",
        "        if self.iteration < self.half_iteration:\n",
        "            lr = self._interpolate(0, self.half_iteration, self.start_lr, self.max_lr)\n",
        "        elif self.iteration < 2 * self.half_iteration:\n",
        "            lr = self._interpolate(\n",
        "                self.half_iteration, 2 * self.half_iteration, self.max_lr, self.start_lr\n",
        "            )\n",
        "        else:\n",
        "            lr = self._interpolate(\n",
        "                2 * self.half_iteration, self.iterations, self.start_lr, self.last_lr\n",
        "            )\n",
        "        self.iteration += 1\n",
        "        K.set_value(self.model.optimizer.learning_rate, lr)\n",
        "\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yltyqrxRuIJo"
      },
      "outputs": [],
      "source": [
        "IMG_HEIGHT = 64 * 2\n",
        "IMG_WIDTH = 48 * 2\n",
        "BATCH_SIZE = 32\n",
        "TRAIN_SIZE = 10000\n",
        "TEST_SIZE = 3000\n",
        "EPOCH = 50\n",
        "DATA_DIR = \"data/maize-crop-diagnose/data/train\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QJxln2mzuIJo",
        "outputId": "9ce4fc0a-368b-4916-a2d6-acb84e200db7"
      },
      "outputs": [],
      "source": [
        "img_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "num_classes = len(img_data.class_names)\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_set = img_data.take(TRAIN_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "test_set = img_data.skip(TRAIN_SIZE).take(TEST_SIZE)\n",
        "val_set = img_data.skip(TRAIN_SIZE).skip(TEST_SIZE).prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68820
        },
        "id": "IO82-w3NuIJp",
        "outputId": "83e5b933-616d-4fc8-b5f1-4136832ae46c"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    K.clear_session()\n",
        "    train_set, val_set = tf.keras.utils.image_dataset_from_directory(\n",
        "        DATA_DIR,\n",
        "        validation_split=0.2,\n",
        "        subset=\"both\",\n",
        "        seed=42,\n",
        "        image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "    num_classes = len(train_set.class_names)\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    train_set = train_set.prefetch(buffer_size=AUTOTUNE).cache()\n",
        "    val_set = val_set.prefetch(buffer_size=AUTOTUNE).cache()\n",
        "    # Hyperparameters for network architecture\n",
        "    ## Number of filters\n",
        "    filters_layer_1 = trial.suggest_categorical(\"filters_layer_1\", [16, 32, 64])\n",
        "    filters_layer_2 = trial.suggest_int(\"prop_filters_layer_2\", 1, 4)\n",
        "    filters_layer_3 = trial.suggest_int(\"prop_filters_layer_3\", 1, 4)\n",
        "    ## Kernel size\n",
        "    kernel_layer_1 = trial.suggest_categorical(\"kernel_layer_1\", [3, 5, 7])\n",
        "    kernel_layer_2 = trial.suggest_categorical(\"kernel_layer_2\", [3, 5, 7])\n",
        "    kernel_layer_3 = trial.suggest_categorical(\"kernel_layer_3\", [3, 5, 7])\n",
        "    ## Activation function\n",
        "    activation = trial.suggest_categorical(\"activation\", [\"elu\", \"selu\"])\n",
        "    if activation == \"selu\":\n",
        "        kernel_initializer = \"lecun_normal\"\n",
        "    else:\n",
        "        kernel_initializer = \"he_normal\"\n",
        "    ## Dropout rate\n",
        "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.25, 0.5)\n",
        "    ## Dense layer size\n",
        "    dense_size = trial.suggest_categorical(\"dense_size\", [16, 32, 64, 128, 256, 512])\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.Rescaling(1.0 / 255),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Conv2D(\n",
        "                filters_layer_1,\n",
        "                kernel_layer_1,\n",
        "                activation=activation,\n",
        "                kernel_initializer=kernel_initializer,\n",
        "            ),\n",
        "            tf.keras.layers.MaxPooling2D(),\n",
        "            tf.keras.layers.SpatialDropout2D(dropout_rate),\n",
        "            tf.keras.layers.Conv2D(\n",
        "                filters_layer_1 * filters_layer_2,\n",
        "                kernel_layer_2,\n",
        "                activation=activation,\n",
        "                kernel_initializer=kernel_initializer,\n",
        "            ),\n",
        "            tf.keras.layers.MaxPooling2D(),\n",
        "            tf.keras.layers.SpatialDropout2D(dropout_rate),\n",
        "            tf.keras.layers.Conv2D(\n",
        "                filters_layer_1 * filters_layer_2 * filters_layer_3,\n",
        "                kernel_layer_3,\n",
        "                activation=activation,\n",
        "                kernel_initializer=kernel_initializer,\n",
        "            ),\n",
        "            tf.keras.layers.MaxPooling2D(),\n",
        "            tf.keras.layers.SpatialDropout2D(dropout_rate),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(\n",
        "                dense_size, activation=activation, kernel_initializer=kernel_initializer\n",
        "            ),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(dropout_rate),\n",
        "            tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
        "        ]\n",
        "    )\n",
        "    # Hyperparameters for OneCycleScheduler\n",
        "    max_lr = trial.suggest_float(\"max_lr\", 1e-3, 1e-1)\n",
        "    start_lr = trial.suggest_float(\"start_lr\", max_lr * 0.01, max_lr * 0.8)\n",
        "    last_lr = trial.suggest_float(\"last_lr\", start_lr, max_lr)\n",
        "    # Model definition\n",
        "    onecycle = OneCycleSchedulerNoMom(\n",
        "        TRAIN_SIZE // BATCH_SIZE * EPOCH,\n",
        "        max_lr=max_lr,\n",
        "        start_lr=start_lr,\n",
        "        last_lr=last_lr,\n",
        "    )\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        patience=5, restore_best_weights=True\n",
        "    )\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.SGD(),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    # Fitting model\n",
        "    history = model.fit(\n",
        "        train_set,\n",
        "        validation_data=val_set,\n",
        "        epochs=EPOCH,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[early_stopping, onecycle],\n",
        "    )\n",
        "    # Evaluating and returning F1 score\n",
        "    loss, acc = model.evaluate(val_set)\n",
        "    return acc\n",
        "\n",
        "\n",
        "study = optuna.create_study(\n",
        "    storage=\"sqlite:///drive/MyDrive/maize-crop-diagnose/db_maize_models.sqlite3\",\n",
        "    study_name=\"cnn_onecycle_sgd\",\n",
        "    direction=\"maximize\",\n",
        "    load_if_exists=True,\n",
        ")\n",
        "\n",
        "study.optimize(objective, n_trials=100)\n",
        "print(f\"Best value: {study.best_value} (params: {study.best_params})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PznrQA7-_NNM"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    K.clear_session()\n",
        "    train_set, val_set = tf.keras.utils.image_dataset_from_directory(\n",
        "        DATA_DIR,\n",
        "        validation_split=0.2,\n",
        "        subset=\"both\",\n",
        "        seed=42,\n",
        "        image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "    num_classes = len(train_set.class_names)\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    train_set = train_set.prefetch(buffer_size=AUTOTUNE).cache()\n",
        "    val_set = val_set.prefetch(buffer_size=AUTOTUNE).cache()\n",
        "\n",
        "    # Network architecture\n",
        "    ## Input\n",
        "    ### Hyperparameters for input\n",
        "    num_input_filter = trial.suggest_int(\"num_input_filter\", 16, 64)\n",
        "    input_kernel_size = trial.suggest_categorical(\"input_kernel_size\", [3, 5, 7])\n",
        "    ### Architecture\n",
        "    input_layer = keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "    conv_layer_1 = keras.layers.Conv2D(\n",
        "        filters=num_input_filter, kernel_size=input_kernel_size, activation=\"relu\"\n",
        "    )(input_layer)\n",
        "    pool_layer_1 = keras.layers.MaxPooling2D()(conv_layer_1)\n",
        "    ## Residual Layers\n",
        "    ### Hyperparameters for Residual\n",
        "    num_residual_filter = trial.suggest_int(\"num_input_filter\", 16, 64)\n",
        "    residual_kernel_size = trial.suggest_categorical(\"input_kernel_size\", [3, 5, 7])\n",
        "    ### Architecture\n",
        "    #### R1\n",
        "    r1_conv_layer1 = keras.layers.Conv2D(\n",
        "        filters=num_residual_filter,\n",
        "        kernel_size=residual_kernel_size,\n",
        "        activation=\"relu\",\n",
        "        padding=\"same\",\n",
        "    )(pool_layer_1)\n",
        "    r1_batch_norm = keras.layers.BatchNormalization()(r1_conv_layer1)\n",
        "    r1_conv_layer2 = keras.layers.Conv2D(\n",
        "        filters=num_residual_filter, kernel_size=residual_kernel_size, padding=\"same\"\n",
        "    )(r1_batch_norm)\n",
        "    r1_batch_norm_2 = keras.layers.BatchNormalization()(r1_conv_layer2)\n",
        "    r1_out = keras.layers.Add()[pool_layer_1, r1_batch_norm_2]\n",
        "    r1_relu = keras.layers.ReLU()(r1_out)\n",
        "    #### R2\n",
        "    r2_conv_layer1 = keras.layers.Conv2D(\n",
        "        filters=num_residual_filter,\n",
        "        kernel_size=residual_kernel_size,\n",
        "        activation=\"relu\",\n",
        "        padding=\"same\",\n",
        "    )(r1_relu)\n",
        "    r2_batch_norm = keras.layers.BatchNormalization()(r2_conv_layer1)\n",
        "    r2_conv_layer2 = keras.layers.Conv2D(\n",
        "        filters=num_residual_filter, kernel_size=residual_kernel_size, padding=\"same\"\n",
        "    )(r2_batch_norm)\n",
        "    r2_batch_norm_2 = keras.layers.BatchNormalization()(r2_conv_layer2)\n",
        "    r2_out = keras.layers.Add()[r1_relu, r2_batch_norm_2]\n",
        "    r2_relu = keras.layers.ReLU()(r2_out)\n",
        "    #### R3\n",
        "    r3_conv_layer1 = keras.layers.Conv2D(\n",
        "        filters=num_residual_filter,\n",
        "        kernel_size=residual_kernel_size,\n",
        "        activation=\"relu\",\n",
        "        padding=\"same\",\n",
        "    )(r2_relu)\n",
        "    r3_batch_norm = keras.layers.BatchNormalization()(r3_conv_layer1)\n",
        "    r3_conv_layer2 = keras.layers.Conv2D(\n",
        "        filters=num_residual_filter, kernel_size=residual_kernel_size, padding=\"same\"\n",
        "    )(r3_batch_norm)\n",
        "    r3_batch_norm_2 = keras.layers.BatchNormalization()(r3_conv_layer2)\n",
        "    r3_out = keras.layers.Add()[r2_relu, r3_batch_norm_2]\n",
        "    r3_relu = keras.layers.ReLU()(r3_out)\n",
        "    #### R4\n",
        "    r4_conv_layer1 = keras.layers.Conv2D(\n",
        "        filters=num_residual_filter,\n",
        "        kernel_size=residual_kernel_size,\n",
        "        activation=\"relu\",\n",
        "        padding=\"same\",\n",
        "    )(r3_relu)\n",
        "    r4_batch_norm = keras.layers.BatchNormalization()(r4_conv_layer1)\n",
        "    r4_conv_layer2 = keras.layers.Conv2D(\n",
        "        filters=num_residual_filter, kernel_size=residual_kernel_size, padding=\"same\"\n",
        "    )(r4_batch_norm)\n",
        "    r4_batch_norm_2 = keras.layers.BatchNormalization()(r4_conv_layer2)\n",
        "    r4_out = keras.layers.Add()[r3_relu, r4_batch_norm_2]\n",
        "    r4_relu = keras.layers.ReLU()(r4_out)\n",
        "\n",
        "    ## Output\n",
        "    ### Hyperparameters for Output\n",
        "    num_output = trial.suggest_int(\"num_input_filter\", 16, 128)\n",
        "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
        "    ### Architecture\n",
        "    pool = keras.layers.MaxPooling2D()(r4_relu)\n",
        "    flatten = keras.layers.Flatten()(pool)\n",
        "    dense = keras.layers.Dense(num_output, activation=\"relu\")(flatten)\n",
        "    dropout = keras.layers.Dropout(dropout_rate)(dense)\n",
        "    output = keras.layers.Dense(num_classes, activation=\"softmax\")(dropout)\n",
        "    # Model definition\n",
        "    model = keras.Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    # Hyperparameters for OneCycleScheduler\n",
        "    max_lr = trial.suggest_float(\"max_lr\", 0.005, 0.5)\n",
        "    start_lr = trial.suggest_float(\"start_lr\", max_lr * 0.01, max_lr * 0.8)\n",
        "    last_lr = trial.suggest_float(\"last_lr\", start_lr, max_lr)\n",
        "    # Model definition\n",
        "    onecycle = OneCycleSchedulerNoMom(\n",
        "        TRAIN_SIZE // BATCH_SIZE * EPOCH,\n",
        "        max_lr=max_lr,\n",
        "        start_lr=start_lr,\n",
        "        last_lr=last_lr,\n",
        "    )\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        patience=5, restore_best_weights=True\n",
        "    )\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.SGD(),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    # Fitting model\n",
        "    history = model.fit(\n",
        "        train_set,\n",
        "        validation_data=val_set,\n",
        "        epochs=EPOCH,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[early_stopping, onecycle],\n",
        "    )\n",
        "    # Evaluating and returning F1 score\n",
        "    loss, acc = model.evaluate(val_set)\n",
        "    return acc\n",
        "\n",
        "\n",
        "study = optuna.create_study(\n",
        "    storage=\"sqlite:///drive/MyDrive/maize-crop-diagnose/db_maize_models.sqlite3\",\n",
        "    study_name=\"resnet_onecycle_sgd\",\n",
        "    direction=\"maximize\",\n",
        "    load_if_exists=True,\n",
        ")\n",
        "\n",
        "study.optimize(objective, n_trials=100)\n",
        "print(f\"Best value: {study.best_value} (params: {study.best_params})\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
