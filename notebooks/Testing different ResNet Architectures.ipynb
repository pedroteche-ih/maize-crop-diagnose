{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os, optuna\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")\n",
    "\n",
    "K = keras.backend\n",
    "# Changing default dir\n",
    "os.chdir(\"/Users/pedroteche/Documents/GitHub/maize-crop-diagnose/\")\n",
    "# optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir 'data'\n",
    "# !cp -r 'drive/MyDrive/maize-crop-diagnose' 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != \"/device:GPU:0\":\n",
    "    raise SystemError(\"GPU device not found\")\n",
    "print(\"Found GPU at: {}\".format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        iterations,\n",
    "        max_lr=1e-3,\n",
    "        start_lr=None,\n",
    "        start_mom=0.95,\n",
    "        min_mom=0.85,\n",
    "        last_iterations=None,\n",
    "        last_lr=None,\n",
    "    ):\n",
    "        self.iterations = iterations\n",
    "        self.max_lr = max_lr\n",
    "        self.start_lr = start_lr or max_lr / 10\n",
    "        self.start_mom = start_mom\n",
    "        self.min_mom = min_mom\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_lr = last_lr or self.start_lr / 1000\n",
    "        self.iteration = 0\n",
    "\n",
    "    def _interpolate(self, iter1, iter2, lr1, lr2):\n",
    "        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1\n",
    "\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            lr = self._interpolate(0, self.half_iteration, self.start_lr, self.max_lr)\n",
    "            mom = self._interpolate(\n",
    "                0, self.half_iteration, self.start_mom, self.min_mom\n",
    "            )\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            lr = self._interpolate(\n",
    "                self.half_iteration, 2 * self.half_iteration, self.max_lr, self.start_lr\n",
    "            )\n",
    "            mom = self._interpolate(\n",
    "                self.half_iteration,\n",
    "                2 * self.half_iteration,\n",
    "                self.min_mom,\n",
    "                self.start_mom,\n",
    "            )\n",
    "        else:\n",
    "            lr = self._interpolate(\n",
    "                2 * self.half_iteration, self.iterations, self.start_lr, self.last_lr\n",
    "            )\n",
    "            mom = self.start_mom\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, lr)\n",
    "        K.set_value(self.model.optimizer.momentum, mom)\n",
    "\n",
    "\n",
    "class OneCycleSchedulerNoMom(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        iterations,\n",
    "        max_lr=1e-3,\n",
    "        start_lr=None,\n",
    "        last_iterations=None,\n",
    "        last_lr=None,\n",
    "    ):\n",
    "        self.iterations = iterations\n",
    "        self.max_lr = max_lr\n",
    "        self.start_lr = start_lr or max_lr / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_lr = last_lr or self.start_lr / 1000\n",
    "        self.iteration = 0\n",
    "\n",
    "    def _interpolate(self, iter1, iter2, lr1, lr2):\n",
    "        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1\n",
    "\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            lr = self._interpolate(0, self.half_iteration, self.start_lr, self.max_lr)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            lr = self._interpolate(\n",
    "                self.half_iteration, 2 * self.half_iteration, self.max_lr, self.start_lr\n",
    "            )\n",
    "        else:\n",
    "            lr = self._interpolate(\n",
    "                2 * self.half_iteration, self.iterations, self.start_lr, self.last_lr\n",
    "            )\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, lr)\n",
    "\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 64 * 2\n",
    "IMG_WIDTH = 48 * 2\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 50\n",
    "# DATA_DIR = \"data/maize-crop-diagnose/data/train\"\n",
    "TRAIN_DATA_DIR = \"/Volumes/DOCK-HD/Data/maize-crop-diagnose/train\"\n",
    "TEST_DATA_DIR = \"/Volumes/DOCK-HD/Data/maize-crop-diagnose/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "K.clear_session()\n",
    "train_set, val_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_DATA_DIR,\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=42,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "num_classes = len(train_set.class_names)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "print(train_set.cardinality().numpy())\n",
    "train_set = train_set.prefetch(buffer_size=AUTOTUNE).cache()\n",
    "val_set = val_set.prefetch(buffer_size=AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 11800\n",
    "TEST_SIZE = 2949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "test_set= tf.keras.utils.image_dataset_from_directory(\n",
    "    TEST_DATA_DIR,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "test_set = test_set.prefetch(buffer_size=AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple input/output example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "rescale = keras.layers.Rescaling(1.0 / 255)(input)\n",
    "conv_in = keras.layers.Conv2D(\n",
    "    filters=64, kernel_size=(7, 7), strides=(2, 2), activation=\"relu\"\n",
    ")(rescale)\n",
    "pool_in = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_out = keras.layers.GlobalAveragePooling2D()(pool_in)\n",
    "dense_out = keras.layers.Dense(512, activation=\"relu\")(pool_out)\n",
    "output = keras.layers.Dense(num_classes, activation=\"softmax\")(dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.SGD(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecycle = OneCycleSchedulerNoMom(\n",
    "    TRAIN_SIZE // BATCH_SIZE * EPOCH,\n",
    "    max_lr=0.1,\n",
    "    start_lr=0.01,\n",
    "    last_lr=0.001,\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(\n",
    "    train_set,\n",
    "    validation_data=val_set,\n",
    "    epochs=EPOCH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, onecycle],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Residual Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "input = keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "rescale = keras.layers.Rescaling(1.0 / 255)(input)\n",
    "conv_in = keras.layers.Conv2D(\n",
    "    filters=64, kernel_size=(7, 7), strides=(2, 2), activation=\"relu\"\n",
    ")(rescale)\n",
    "pool_in = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding = \"same\")(conv_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_r1_1 = keras.layers.Conv2D(64, 3, 1, padding = \"same\")(pool_in)\n",
    "bn_r1_1 = keras.layers.BatchNormalization()(conv_r1_1)\n",
    "relu_r1_1 = keras.layers.ReLU()(bn_r1_1)\n",
    "conv_r1_2 = keras.layers.Conv2D(64, 3, 1, padding = \"same\")(relu_r1_1)\n",
    "bn_r1_2 = keras.layers.BatchNormalization()(conv_r1_2)\n",
    "skip_r1 = keras.layers.Add()([bn_r1_2, pool_in])\n",
    "relu_r1_2 = keras.layers.ReLU()(skip_r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_out = keras.layers.GlobalAveragePooling2D()(relu_r1_2)\n",
    "dense_out = keras.layers.Dense(512, activation=\"relu\")(pool_out)\n",
    "output = keras.layers.Dense(num_classes, activation=\"softmax\")(dense_out)\n",
    "\n",
    "model_1r = keras.Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecycle = OneCycleSchedulerNoMom(\n",
    "    TRAIN_SIZE // BATCH_SIZE * EPOCH,\n",
    "    max_lr=0.1,\n",
    "    start_lr=0.01,\n",
    "    last_lr=0.001,\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, start_from_epoch=20)\n",
    "\n",
    "model_1r.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.SGD(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_1r.fit(\n",
    "    train_set,\n",
    "    validation_data=val_set,\n",
    "    epochs=EPOCH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, onecycle],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Residual Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "# Input layers\n",
    "input = keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "rescale = keras.layers.Rescaling(1.0 / 255)(input)\n",
    "conv_in = keras.layers.Conv2D(\n",
    "    filters=64, kernel_size=(7, 7), strides=(2, 2), activation=\"relu\"\n",
    ")(rescale)\n",
    "pool_in = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding = \"same\")(conv_in)\n",
    "# R1\n",
    "conv_r1_1 = keras.layers.Conv2D(64, 3, 1, padding = \"same\")(pool_in)\n",
    "bn_r1_1 = keras.layers.BatchNormalization()(conv_r1_1)\n",
    "relu_r1_1 = keras.layers.ReLU()(bn_r1_1)\n",
    "conv_r1_2 = keras.layers.Conv2D(64, 3, 1, padding = \"same\")(relu_r1_1)\n",
    "bn_r1_2 = keras.layers.BatchNormalization()(conv_r1_2)\n",
    "skip_r1 = keras.layers.Add()([bn_r1_2, pool_in])\n",
    "relu_r1_2 = keras.layers.ReLU()(skip_r1)\n",
    "# R2\n",
    "conv_r2_1 = keras.layers.Conv2D(64, 3, 1, padding = \"same\")(relu_r1_2)\n",
    "bn_r2_1 = keras.layers.BatchNormalization()(conv_r2_1)\n",
    "relu_r2_1 = keras.layers.ReLU()(bn_r2_1)\n",
    "conv_r2_2 = keras.layers.Conv2D(64, 3, 1, padding = \"same\")(relu_r2_1)\n",
    "bn_r2_2 = keras.layers.BatchNormalization()(conv_r2_2)\n",
    "skip_r2 = keras.layers.Add()([bn_r2_2, relu_r1_2])\n",
    "relu_r2_2 = keras.layers.ReLU()(skip_r2)\n",
    "# Output layers\n",
    "pool_out = keras.layers.GlobalAveragePooling2D()(relu_r2_2)\n",
    "dense_out = keras.layers.Dense(512, activation=\"relu\")(pool_out)\n",
    "output = keras.layers.Dense(num_classes, activation=\"softmax\")(dense_out)\n",
    "# Model\n",
    "model_2r = keras.Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecycle = OneCycleSchedulerNoMom(\n",
    "    TRAIN_SIZE // BATCH_SIZE * EPOCH,\n",
    "    max_lr=0.1,\n",
    "    start_lr=0.01,\n",
    "    last_lr=0.001,\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, start_from_epoch=20)\n",
    "\n",
    "model_2r.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.SGD(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_2r.fit(\n",
    "    train_set,\n",
    "    validation_data=val_set,\n",
    "    epochs=EPOCH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, onecycle],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Residual Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "# Input layers\n",
    "input = keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "rescale = keras.layers.Rescaling(1.0 / 255)(input)\n",
    "conv_in = keras.layers.Conv2D(\n",
    "    filters=64, kernel_size=(7, 7), strides=(2, 2), activation=\"relu\"\n",
    ")(rescale)\n",
    "pool_in = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding = \"same\")(conv_in)\n",
    "# R1\n",
    "conv_r1_1 = keras.layers.Conv2D(64, 3, 1, padding = \"same\")(pool_in)\n",
    "bn_r1_1 = keras.layers.BatchNormalization()(conv_r1_1)\n",
    "relu_r1_1 = keras.layers.ReLU()(bn_r1_1)\n",
    "conv_r1_2 = keras.layers.Conv2D(64, 3, 1, padding = \"same\")(relu_r1_1)\n",
    "bn_r1_2 = keras.layers.BatchNormalization()(conv_r1_2)\n",
    "skip_r1 = keras.layers.Add()([bn_r1_2, pool_in])\n",
    "relu_r1_2 = keras.layers.ReLU()(skip_r1)\n",
    "# R2\n",
    "conv_r2_1 = keras.layers.Conv2D(64, 3, 1, padding = \"same\")(relu_r1_2)\n",
    "bn_r2_1 = keras.layers.BatchNormalization()(conv_r2_1)\n",
    "relu_r2_1 = keras.layers.ReLU()(bn_r2_1)\n",
    "conv_r2_2 = keras.layers.Conv2D(64, 3, 1, padding = \"same\")(relu_r2_1)\n",
    "bn_r2_2 = keras.layers.BatchNormalization()(conv_r2_2)\n",
    "skip_r2 = keras.layers.Add()([bn_r2_2, relu_r1_2])\n",
    "relu_r2_2 = keras.layers.ReLU()(skip_r2)\n",
    "# R3\n",
    "conv_r3_skip = keras.layers.Conv2D(128, 1, 2, padding = \"same\")(relu_r2_2)\n",
    "conv_r3_1 = keras.layers.Conv2D(128, 3, 2, padding = \"same\")(relu_r2_2)\n",
    "bn_r3_1 = keras.layers.BatchNormalization()(conv_r3_1)\n",
    "relu_r3_1 = keras.layers.ReLU()(bn_r3_1)\n",
    "conv_r3_2 = keras.layers.Conv2D(128, 3, 1, padding = \"same\")(relu_r3_1)\n",
    "bn_r3_2 = keras.layers.BatchNormalization()(conv_r3_2)\n",
    "skip_r3 = keras.layers.Add()([bn_r3_2, conv_r3_skip])\n",
    "relu_r3_2 = keras.layers.ReLU()(skip_r3)\n",
    "# Output layers\n",
    "pool_out = keras.layers.GlobalAveragePooling2D()(relu_r3_2)\n",
    "dense_out = keras.layers.Dense(512, activation=\"relu\")(pool_out)\n",
    "output = keras.layers.Dense(num_classes, activation=\"softmax\")(dense_out)\n",
    "# Model\n",
    "model_3r = keras.Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecycle = OneCycleSchedulerNoMom(\n",
    "    TRAIN_SIZE // BATCH_SIZE * EPOCH,\n",
    "    max_lr=0.1,\n",
    "    start_lr=0.01,\n",
    "    last_lr=0.001,\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, start_from_epoch=20)\n",
    "\n",
    "model_3r.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.SGD(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_3r.fit(\n",
    "    train_set,\n",
    "    validation_data=val_set,\n",
    "    epochs=EPOCH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, onecycle],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optuna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
