{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os, optuna\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "K = keras.backend\n",
    "# Changing default dir\n",
    "# os.chdir(\"/Users/pedroteche/Documents/GitHub/maize-crop-diagnose/\")\n",
    "# optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir 'data'\n",
    "!cp -r 'drive/MyDrive/maize-crop-diagnose' 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != \"/device:GPU:0\":\n",
    "    raise SystemError(\"GPU device not found\")\n",
    "print(\"Found GPU at: {}\".format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        iterations,\n",
    "        max_lr=1e-3,\n",
    "        start_lr=None,\n",
    "        start_mom=0.95,\n",
    "        min_mom=0.85,\n",
    "        last_iterations=None,\n",
    "        last_lr=None,\n",
    "    ):\n",
    "        self.iterations = iterations\n",
    "        self.max_lr = max_lr\n",
    "        self.start_lr = start_lr or max_lr / 10\n",
    "        self.start_mom = start_mom\n",
    "        self.min_mom = min_mom\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_lr = last_lr or self.start_lr / 1000\n",
    "        self.iteration = 0\n",
    "\n",
    "    def _interpolate(self, iter1, iter2, lr1, lr2):\n",
    "        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1\n",
    "\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            lr = self._interpolate(0, self.half_iteration, self.start_lr, self.max_lr)\n",
    "            mom = self._interpolate(\n",
    "                0, self.half_iteration, self.start_mom, self.min_mom\n",
    "            )\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            lr = self._interpolate(\n",
    "                self.half_iteration, 2 * self.half_iteration, self.max_lr, self.start_lr\n",
    "            )\n",
    "            mom = self._interpolate(\n",
    "                self.half_iteration,\n",
    "                2 * self.half_iteration,\n",
    "                self.min_mom,\n",
    "                self.start_mom,\n",
    "            )\n",
    "        else:\n",
    "            lr = self._interpolate(\n",
    "                2 * self.half_iteration, self.iterations, self.start_lr, self.last_lr\n",
    "            )\n",
    "            mom = self.start_mom\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, lr)\n",
    "        K.set_value(self.model.optimizer.momentum, mom)\n",
    "\n",
    "\n",
    "class OneCycleSchedulerNoMom(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        iterations,\n",
    "        max_lr=1e-3,\n",
    "        start_lr=None,\n",
    "        last_iterations=None,\n",
    "        last_lr=None,\n",
    "    ):\n",
    "        self.iterations = iterations\n",
    "        self.max_lr = max_lr\n",
    "        self.start_lr = start_lr or max_lr / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_lr = last_lr or self.start_lr / 1000\n",
    "        self.iteration = 0\n",
    "\n",
    "    def _interpolate(self, iter1, iter2, lr1, lr2):\n",
    "        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1\n",
    "\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            lr = self._interpolate(0, self.half_iteration, self.start_lr, self.max_lr)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            lr = self._interpolate(\n",
    "                self.half_iteration, 2 * self.half_iteration, self.max_lr, self.start_lr\n",
    "            )\n",
    "        else:\n",
    "            lr = self._interpolate(\n",
    "                2 * self.half_iteration, self.iterations, self.start_lr, self.last_lr\n",
    "            )\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, lr)\n",
    "\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 64 * 2\n",
    "IMG_WIDTH = 48 * 2\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 50\n",
    "TRAIN_DATA_DIR = \"data/maize-crop-diagnose/data/train\"\n",
    "TEST_DATA_DIR = \"data/maize-crop-diagnose/data/test\"\n",
    "# TRAIN_DATA_DIR = \"/Volumes/DOCK-HD/Data/maize-crop-diagnose/train\"\n",
    "# TEST_DATA_DIR = \"/Volumes/DOCK-HD/Data/maize-crop-diagnose/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14749 files belonging to 3 classes.\n",
      "Using 11800 files for training.\n",
      "Using 2949 files for validation.\n",
      "369\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_DATA_DIR,\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=42,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "num_classes = len(train_set.class_names)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_set = train_set.prefetch(buffer_size=AUTOTUNE).cache()\n",
    "val_set = val_set.prefetch(buffer_size=AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 11800\n",
    "TEST_SIZE = 2949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    TEST_DATA_DIR,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "test_set = test_set.prefetch(buffer_size=AUTOTUNE).cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    K.clear_session()\n",
    "    train_set, val_set = tf.keras.utils.image_dataset_from_directory(\n",
    "        TRAIN_DATA_DIR,\n",
    "        validation_split=0.2,\n",
    "        subset=\"both\",\n",
    "        seed=42,\n",
    "        image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    num_classes = len(train_set.class_names)\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_set = train_set.prefetch(buffer_size=AUTOTUNE).cache()\n",
    "    val_set = val_set.prefetch(buffer_size=AUTOTUNE).cache()\n",
    "\n",
    "    # Architecture Hyperparameters\n",
    "    size_dense = trial.suggest_int(\"size_dense\", 128, 1024, 32)\n",
    "    activation_function = trial.suggest_categorical(\"activation_function\", [\"relu\", \"elu\"])\n",
    "    # Input layers\n",
    "    input = keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    rescale = keras.layers.Rescaling(1.0 / 255)(input)\n",
    "    conv_in = keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=(7, 7), strides=(2, 2), activation=activation_function\n",
    "    )(rescale)\n",
    "    pool_in = keras.layers.MaxPooling2D(\n",
    "        pool_size=(3, 3), strides=(2, 2), padding=\"same\"\n",
    "    )(conv_in)\n",
    "    # R1\n",
    "    conv_r1_1 = keras.layers.Conv2D(64, 3, 1, padding=\"same\")(pool_in)\n",
    "    bn_r1_1 = keras.layers.BatchNormalization()(conv_r1_1)\n",
    "    if activation_function == \"relu\":\n",
    "        relu_r1_1 = keras.layers.ReLU()(bn_r1_1)\n",
    "    else:\n",
    "        relu_r1_1 = keras.layers.ELU()(bn_r1_1)\n",
    "    conv_r1_2 = keras.layers.Conv2D(64, 3, 1, padding=\"same\")(relu_r1_1)\n",
    "    bn_r1_2 = keras.layers.BatchNormalization()(conv_r1_2)\n",
    "    skip_r1 = keras.layers.Add()([bn_r1_2, pool_in])\n",
    "    if activation_function == \"relu\":\n",
    "        relu_r1_2 = keras.layers.ReLU()(skip_r1)\n",
    "    else:\n",
    "        relu_r1_2 = keras.layers.ELU()(skip_r1)\n",
    "    # R2\n",
    "    conv_r2_1 = keras.layers.Conv2D(64, 3, 1, padding=\"same\")(relu_r1_2)\n",
    "    bn_r2_1 = keras.layers.BatchNormalization()(conv_r2_1)\n",
    "    if activation_function == \"relu\":\n",
    "        relu_r2_1 = keras.layers.ReLU()(bn_r2_1)\n",
    "    else:\n",
    "        relu_r2_1 = keras.layers.ELU()(bn_r2_1)\n",
    "    conv_r2_2 = keras.layers.Conv2D(64, 3, 1, padding=\"same\")(relu_r2_1)\n",
    "    bn_r2_2 = keras.layers.BatchNormalization()(conv_r2_2)\n",
    "    skip_r2 = keras.layers.Add()([bn_r2_2, relu_r1_2])\n",
    "    if activation_function == \"relu\":\n",
    "        relu_r2_2 = keras.layers.ReLU()(skip_r2)\n",
    "    else:\n",
    "        relu_r2_2 = keras.layers.ELU()(skip_r2)\n",
    "    # R3\n",
    "    conv_r3_skip = keras.layers.Conv2D(128, 1, 2, padding=\"same\")(relu_r2_2)\n",
    "    conv_r3_1 = keras.layers.Conv2D(128, 3, 2, padding=\"same\")(relu_r2_2)\n",
    "    bn_r3_1 = keras.layers.BatchNormalization()(conv_r3_1)\n",
    "    if activation_function == \"relu\":\n",
    "        relu_r3_1 = keras.layers.ReLU()(bn_r3_1)\n",
    "    else:\n",
    "        relu_r3_1 = keras.layers.ELU()(bn_r3_1)\n",
    "    conv_r3_2 = keras.layers.Conv2D(128, 3, 1, padding=\"same\")(relu_r3_1)\n",
    "    bn_r3_2 = keras.layers.BatchNormalization()(conv_r3_2)\n",
    "    skip_r3 = keras.layers.Add()([bn_r3_2, conv_r3_skip])\n",
    "    if activation_function == \"relu\":\n",
    "        relu_r3_2 = keras.layers.ReLU()(skip_r3)\n",
    "    else:\n",
    "        relu_r3_2 = keras.layers.ELU()(skip_r3)\n",
    "    # Output layers\n",
    "    pool_out = keras.layers.GlobalAveragePooling2D()(relu_r3_2)\n",
    "    dense_out = keras.layers.Dense(size_dense, activation=activation_function)(pool_out)\n",
    "    output = keras.layers.Dense(num_classes, activation=\"softmax\")(dense_out)\n",
    "    # Model\n",
    "    model = keras.Model(inputs=input, outputs=output)\n",
    "    # Fitting model\n",
    "    \n",
    "    max_lr = trial.suggest_float(\"max_lr\", 0.01, 0.2)\n",
    "    start_lr_prop = trial.suggest_float(\"start_lr\", 1./50, 1./5)\n",
    "    last_lr = trial.suggest_float(\"last_lr\", 0.0001, 0.01)\n",
    "    onecycle = OneCycleSchedulerNoMom(\n",
    "        TRAIN_SIZE // BATCH_SIZE * EPOCH,\n",
    "        max_lr=max_lr,\n",
    "        start_lr=start_lr_prop * max_lr,\n",
    "        last_lr=last_lr,\n",
    "    )\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        patience=10, restore_best_weights=True, start_from_epoch=20\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.legacy.SGD(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        train_set,\n",
    "        validation_data=val_set,\n",
    "        epochs=EPOCH,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[early_stopping, onecycle],\n",
    "    )\n",
    "    # Evaluating and returning F1 score\n",
    "    loss, acc = model.evaluate(val_set)\n",
    "    return acc\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    storage=\"sqlite:///drive/MyDrive/maize-crop-diagnose/db_maize_models.sqlite3\",\n",
    "    study_name=\"resnet_onecycle_sgd\",\n",
    "    direction=\"maximize\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(f\"Best value: {study.best_value} (params: {study.best_params})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optuna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
