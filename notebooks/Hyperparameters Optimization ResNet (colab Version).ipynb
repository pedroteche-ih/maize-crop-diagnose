{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmqenyfP_04J",
        "outputId": "c1f0d1fc-25d1-49e3-b132-8861adb415f3"
      },
      "outputs": [],
      "source": [
        "!pip install optuna, python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giBcDSSS_04K",
        "outputId": "be344a58-3ce3-4942-9721-42ba04b5e0dc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os, optuna\n",
        "import warnings\n",
        "from PIL import Image\n",
        "from tensorflow import keras\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(dotenv_path=\"local_paths.env\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "K = keras.backend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfT_eF0O_04K"
      },
      "outputs": [],
      "source": [
        "!mkdir 'data'\n",
        "!cp -r {os.getenv(\"GOOGLE_DRIVE_PATH\")} 'data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg2T6JWc_04K",
        "outputId": "de016465-a4ab-4d87-f688-a26fb9149238"
      },
      "outputs": [],
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != \"/device:GPU:0\":\n",
        "    raise SystemError(\"GPU device not found\")\n",
        "print(\"Found GPU at: {}\".format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bip85tCg_04L"
      },
      "outputs": [],
      "source": [
        "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        iterations,\n",
        "        max_lr=1e-3,\n",
        "        start_lr=None,\n",
        "        start_mom=0.95,\n",
        "        min_mom=0.85,\n",
        "        last_iterations=None,\n",
        "        last_lr=None,\n",
        "    ):\n",
        "        self.iterations = iterations\n",
        "        self.max_lr = max_lr\n",
        "        self.start_lr = start_lr or max_lr / 10\n",
        "        self.start_mom = start_mom\n",
        "        self.min_mom = min_mom\n",
        "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
        "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
        "        self.last_lr = last_lr or self.start_lr / 1000\n",
        "        self.iteration = 0\n",
        "\n",
        "    def _interpolate(self, iter1, iter2, lr1, lr2):\n",
        "        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1\n",
        "\n",
        "    def on_batch_begin(self, batch, logs):\n",
        "        if self.iteration < self.half_iteration:\n",
        "            lr = self._interpolate(0, self.half_iteration, self.start_lr, self.max_lr)\n",
        "            mom = self._interpolate(\n",
        "                0, self.half_iteration, self.start_mom, self.min_mom\n",
        "            )\n",
        "        elif self.iteration < 2 * self.half_iteration:\n",
        "            lr = self._interpolate(\n",
        "                self.half_iteration, 2 * self.half_iteration, self.max_lr, self.start_lr\n",
        "            )\n",
        "            mom = self._interpolate(\n",
        "                self.half_iteration,\n",
        "                2 * self.half_iteration,\n",
        "                self.min_mom,\n",
        "                self.start_mom,\n",
        "            )\n",
        "        else:\n",
        "            lr = self._interpolate(\n",
        "                2 * self.half_iteration, self.iterations, self.start_lr, self.last_lr\n",
        "            )\n",
        "            mom = self.start_mom\n",
        "        self.iteration += 1\n",
        "        K.set_value(self.model.optimizer.learning_rate, lr)\n",
        "        K.set_value(self.model.optimizer.momentum, mom)\n",
        "\n",
        "\n",
        "class OneCycleSchedulerNoMom(tf.keras.callbacks.Callback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        iterations,\n",
        "        max_lr=1e-3,\n",
        "        start_lr=None,\n",
        "        last_iterations=None,\n",
        "        last_lr=None,\n",
        "    ):\n",
        "        self.iterations = iterations\n",
        "        self.max_lr = max_lr\n",
        "        self.start_lr = start_lr or max_lr / 10\n",
        "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
        "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
        "        self.last_lr = last_lr or self.start_lr / 1000\n",
        "        self.iteration = 0\n",
        "\n",
        "    def _interpolate(self, iter1, iter2, lr1, lr2):\n",
        "        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1\n",
        "\n",
        "    def on_batch_begin(self, batch, logs):\n",
        "        if self.iteration < self.half_iteration:\n",
        "            lr = self._interpolate(0, self.half_iteration, self.start_lr, self.max_lr)\n",
        "        elif self.iteration < 2 * self.half_iteration:\n",
        "            lr = self._interpolate(\n",
        "                self.half_iteration, 2 * self.half_iteration, self.max_lr, self.start_lr\n",
        "            )\n",
        "        else:\n",
        "            lr = self._interpolate(\n",
        "                2 * self.half_iteration, self.iterations, self.start_lr, self.last_lr\n",
        "            )\n",
        "        self.iteration += 1\n",
        "        K.set_value(self.model.optimizer.learning_rate, lr)\n",
        "\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCvCAh8t_04L"
      },
      "outputs": [],
      "source": [
        "IMG_HEIGHT = 64 * 2\n",
        "IMG_WIDTH = 48 * 2\n",
        "BATCH_SIZE = 32\n",
        "EPOCH = 50\n",
        "DATA_DIR = \"data/maize-crop-diagnose/data/train\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cztSdBH_04L",
        "outputId": "ecbba5fe-f053-4525-d96b-44fe02c5e126"
      },
      "outputs": [],
      "source": [
        "val_set = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR,\n",
        "    validation_split=0.2,\n",
        "    subset=\"both\",\n",
        "    seed=42,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "class_names = train_test_set.class_names\n",
        "num_classes = len(train_test_set.class_names)\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w2aiuU2_04M"
      },
      "outputs": [],
      "source": [
        "TRAIN_SIZE = 11800"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf5DI70X_04M"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_Ja1o5g_04N",
        "outputId": "da5d84a5-ba2e-49f2-e4ea-c18db1e64a40"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    K.clear_session()\n",
        "    train_set, val_set = tf.keras.utils.image_dataset_from_directory(\n",
        "        TRAIN_DATA_DIR,\n",
        "        validation_split=0.2,\n",
        "        subset=\"both\",\n",
        "        seed=42,\n",
        "        image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "    num_classes = len(train_set.class_names)\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    train_set = train_set.prefetch(buffer_size=AUTOTUNE).cache()\n",
        "    val_set = val_set.prefetch(buffer_size=AUTOTUNE).cache()\n",
        "\n",
        "    # Architecture Hyperparameters\n",
        "    size_dense = trial.suggest_int(\"size_dense\", 128, 1024, 32)\n",
        "    activation_function = trial.suggest_categorical(\"activation_function\", [\"relu\", \"elu\"])\n",
        "    # Input layers\n",
        "    input = keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "    rescale = keras.layers.Rescaling(1.0 / 255)(input)\n",
        "    conv_in = keras.layers.Conv2D(\n",
        "        filters=64, kernel_size=(7, 7), strides=(2, 2), activation=activation_function\n",
        "    )(rescale)\n",
        "    pool_in = keras.layers.MaxPooling2D(\n",
        "        pool_size=(3, 3), strides=(2, 2), padding=\"same\"\n",
        "    )(conv_in)\n",
        "    # R1\n",
        "    conv_r1_1 = keras.layers.Conv2D(64, 3, 1, padding=\"same\")(pool_in)\n",
        "    bn_r1_1 = keras.layers.BatchNormalization()(conv_r1_1)\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r1_1 = keras.layers.ReLU()(bn_r1_1)\n",
        "    else:\n",
        "        relu_r1_1 = keras.layers.ELU()(bn_r1_1)\n",
        "    conv_r1_2 = keras.layers.Conv2D(64, 3, 1, padding=\"same\")(relu_r1_1)\n",
        "    bn_r1_2 = keras.layers.BatchNormalization()(conv_r1_2)\n",
        "    skip_r1 = keras.layers.Add()([bn_r1_2, pool_in])\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r1_2 = keras.layers.ReLU()(skip_r1)\n",
        "    else:\n",
        "        relu_r1_2 = keras.layers.ELU()(skip_r1)\n",
        "    # R2\n",
        "    conv_r2_1 = keras.layers.Conv2D(64, 3, 1, padding=\"same\")(relu_r1_2)\n",
        "    bn_r2_1 = keras.layers.BatchNormalization()(conv_r2_1)\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r2_1 = keras.layers.ReLU()(bn_r2_1)\n",
        "    else:\n",
        "        relu_r2_1 = keras.layers.ELU()(bn_r2_1)\n",
        "    conv_r2_2 = keras.layers.Conv2D(64, 3, 1, padding=\"same\")(relu_r2_1)\n",
        "    bn_r2_2 = keras.layers.BatchNormalization()(conv_r2_2)\n",
        "    skip_r2 = keras.layers.Add()([bn_r2_2, relu_r1_2])\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r2_2 = keras.layers.ReLU()(skip_r2)\n",
        "    else:\n",
        "        relu_r2_2 = keras.layers.ELU()(skip_r2)\n",
        "    # R3\n",
        "    conv_r3_skip = keras.layers.Conv2D(128, 1, 2, padding=\"same\")(relu_r2_2)\n",
        "    conv_r3_1 = keras.layers.Conv2D(128, 3, 2, padding=\"same\")(relu_r2_2)\n",
        "    bn_r3_1 = keras.layers.BatchNormalization()(conv_r3_1)\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r3_1 = keras.layers.ReLU()(bn_r3_1)\n",
        "    else:\n",
        "        relu_r3_1 = keras.layers.ELU()(bn_r3_1)\n",
        "    conv_r3_2 = keras.layers.Conv2D(128, 3, 1, padding=\"same\")(relu_r3_1)\n",
        "    bn_r3_2 = keras.layers.BatchNormalization()(conv_r3_2)\n",
        "    skip_r3 = keras.layers.Add()([bn_r3_2, conv_r3_skip])\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r3_2 = keras.layers.ReLU()(skip_r3)\n",
        "    else:\n",
        "        relu_r3_2 = keras.layers.ELU()(skip_r3)\n",
        "    # Output layers\n",
        "    pool_out = keras.layers.GlobalAveragePooling2D()(relu_r3_2)\n",
        "    dense_out = keras.layers.Dense(size_dense, activation=activation_function)(pool_out)\n",
        "    output = keras.layers.Dense(num_classes, activation=\"softmax\")(dense_out)\n",
        "    # Model\n",
        "    model = keras.Model(inputs=input, outputs=output)\n",
        "    # Fitting model\n",
        "    \n",
        "    max_lr = trial.suggest_float(\"max_lr\", 0.01, 0.2)\n",
        "    start_lr_prop = trial.suggest_float(\"start_lr_prop\", 1./50, 1./5)\n",
        "    last_lr = trial.suggest_float(\"last_lr\", 0.0001, 0.01)\n",
        "    onecycle = OneCycleSchedulerNoMom(\n",
        "        TRAIN_SIZE // BATCH_SIZE * EPOCH,\n",
        "        max_lr=max_lr,\n",
        "        start_lr=start_lr_prop * max_lr,\n",
        "        last_lr=last_lr,\n",
        "    )\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        patience=10, restore_best_weights=True, start_from_epoch=20\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.legacy.SGD(),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        train_set,\n",
        "        validation_data=val_set,\n",
        "        epochs=EPOCH,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[early_stopping, onecycle],\n",
        "    )\n",
        "    # Evaluating and returning F1 score\n",
        "    loss, acc = model.evaluate(val_set)\n",
        "    return acc\n",
        "\n",
        "\n",
        "study = optuna.create_study(\n",
        "    storage=\"sqlite:///drive/MyDrive/maize-crop-diagnose/db_maize_models.sqlite3\",\n",
        "    study_name=\"resnet_onecycle_sgd\",\n",
        "    direction=\"maximize\",\n",
        "    load_if_exists=True,\n",
        ")\n",
        "\n",
        "study.optimize(objective, n_trials=100)\n",
        "print(f\"Best value: {study.best_value} (params: {study.best_params})\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pAS2wO3wPDTn"
      },
      "source": [
        "# Test with droput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bD8C3xhLw_0",
        "outputId": "31692b89-516c-41ff-c815-edbafddf4a00"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    K.clear_session()\n",
        "    train_set, val_set = tf.keras.utils.image_dataset_from_directory(\n",
        "        TRAIN_DATA_DIR,\n",
        "        validation_split=0.2,\n",
        "        subset=\"both\",\n",
        "        seed=42,\n",
        "        image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "    num_classes = len(train_set.class_names)\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    train_set = train_set.prefetch(buffer_size=AUTOTUNE).cache()\n",
        "    val_set = val_set.prefetch(buffer_size=AUTOTUNE).cache()\n",
        "\n",
        "    # Architecture Hyperparameters\n",
        "    size_dense = trial.suggest_int(\"size_dense\", 128, 1024, 32)\n",
        "    activation_function = trial.suggest_categorical(\"activation_function\", [\"relu\", \"elu\"])\n",
        "    droput_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.5)\n",
        "    # Input layers\n",
        "    input = keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "    rescale = keras.layers.Rescaling(1.0 / 255)(input)\n",
        "    conv_in = keras.layers.Conv2D(\n",
        "        filters=64, kernel_size=(7, 7), strides=(2, 2), activation=activation_function\n",
        "    )(rescale)\n",
        "    pool_in = keras.layers.MaxPooling2D(\n",
        "        pool_size=(3, 3), strides=(2, 2), padding=\"same\"\n",
        "    )(conv_in)\n",
        "    # R1\n",
        "    conv_r1_1 = keras.layers.Conv2D(64, 3, 1, padding=\"same\")(pool_in)\n",
        "    bn_r1_1 = keras.layers.BatchNormalization()(conv_r1_1)\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r1_1 = keras.layers.ReLU()(bn_r1_1)\n",
        "    else:\n",
        "        relu_r1_1 = keras.layers.ELU()(bn_r1_1)\n",
        "    conv_r1_2 = keras.layers.Conv2D(64, 3, 1, padding=\"same\")(relu_r1_1)\n",
        "    bn_r1_2 = keras.layers.BatchNormalization()(conv_r1_2)\n",
        "    skip_r1 = keras.layers.Add()([bn_r1_2, pool_in])\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r1_2 = keras.layers.ReLU()(skip_r1)\n",
        "    else:\n",
        "        relu_r1_2 = keras.layers.ELU()(skip_r1)\n",
        "    # R2\n",
        "    conv_r2_1 = keras.layers.Conv2D(64, 3, 1, padding=\"same\")(relu_r1_2)\n",
        "    bn_r2_1 = keras.layers.BatchNormalization()(conv_r2_1)\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r2_1 = keras.layers.ReLU()(bn_r2_1)\n",
        "    else:\n",
        "        relu_r2_1 = keras.layers.ELU()(bn_r2_1)\n",
        "    conv_r2_2 = keras.layers.Conv2D(64, 3, 1, padding=\"same\")(relu_r2_1)\n",
        "    bn_r2_2 = keras.layers.BatchNormalization()(conv_r2_2)\n",
        "    skip_r2 = keras.layers.Add()([bn_r2_2, relu_r1_2])\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r2_2 = keras.layers.ReLU()(skip_r2)\n",
        "    else:\n",
        "        relu_r2_2 = keras.layers.ELU()(skip_r2)\n",
        "    # R3\n",
        "    conv_r3_skip = keras.layers.Conv2D(128, 1, 2, padding=\"same\")(relu_r2_2)\n",
        "    conv_r3_1 = keras.layers.Conv2D(128, 3, 2, padding=\"same\")(relu_r2_2)\n",
        "    bn_r3_1 = keras.layers.BatchNormalization()(conv_r3_1)\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r3_1 = keras.layers.ReLU()(bn_r3_1)\n",
        "    else:\n",
        "        relu_r3_1 = keras.layers.ELU()(bn_r3_1)\n",
        "    conv_r3_2 = keras.layers.Conv2D(128, 3, 1, padding=\"same\")(relu_r3_1)\n",
        "    bn_r3_2 = keras.layers.BatchNormalization()(conv_r3_2)\n",
        "    skip_r3 = keras.layers.Add()([bn_r3_2, conv_r3_skip])\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r3_2 = keras.layers.ReLU()(skip_r3)\n",
        "    else:\n",
        "        relu_r3_2 = keras.layers.ELU()(skip_r3)\n",
        "    # Output layers\n",
        "    pool_out = keras.layers.GlobalAveragePooling2D()(relu_r3_2)\n",
        "    dense_out = keras.layers.Dense(size_dense, activation=activation_function)(pool_out)\n",
        "    dropout = keras.layers.Dropout(droput_rate)\n",
        "    output = keras.layers.Dense(num_classes, activation=\"softmax\")(dense_out)\n",
        "    # Model\n",
        "    model = keras.Model(inputs=input, outputs=output)\n",
        "    # Fitting model\n",
        "    \n",
        "    max_lr = trial.suggest_float(\"max_lr\", 0.01, 0.2)\n",
        "    start_lr_prop = trial.suggest_float(\"start_lr_prop\", 1./50, 1./5)\n",
        "    last_lr = trial.suggest_float(\"last_lr\", 0.0001, 0.01)\n",
        "    onecycle = OneCycleSchedulerNoMom(\n",
        "        TRAIN_SIZE // BATCH_SIZE * EPOCH,\n",
        "        max_lr=max_lr,\n",
        "        start_lr=start_lr_prop * max_lr,\n",
        "        last_lr=last_lr,\n",
        "    )\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        patience=10, restore_best_weights=True, start_from_epoch=20\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.legacy.SGD(),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        train_set,\n",
        "        validation_data=val_set,\n",
        "        epochs=EPOCH,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[early_stopping, onecycle],\n",
        "    )\n",
        "    # Evaluating and returning F1 score\n",
        "    loss, acc = model.evaluate(val_set)\n",
        "    return acc\n",
        "\n",
        "\n",
        "study = optuna.create_study(\n",
        "    storage=\"sqlite:///drive/MyDrive/maize-crop-diagnose/db_maize_models.sqlite3\",\n",
        "    study_name=\"resnet_onecycle_drop_sgd\",\n",
        "    direction=\"maximize\",\n",
        "    load_if_exists=True,\n",
        ")\n",
        "\n",
        "study.optimize(objective, n_trials=100)\n",
        "print(f\"Best value: {study.best_value} (params: {study.best_params})\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sP14tQlvAJk6"
      },
      "source": [
        "# No Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZqdjdM_GzNY"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    K.clear_session()\n",
        "    train_set, val_set = tf.keras.utils.image_dataset_from_directory(\n",
        "        TRAIN_DATA_DIR,\n",
        "        validation_split=0.2,\n",
        "        subset=\"both\",\n",
        "        seed=42,\n",
        "        image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "    num_classes = len(train_set.class_names)\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    train_set = train_set.prefetch(buffer_size=AUTOTUNE).cache()\n",
        "    val_set = val_set.prefetch(buffer_size=AUTOTUNE).cache()\n",
        "\n",
        "    # Architecture Hyperparameters\n",
        "    size_dense = trial.suggest_int(\"size_dense\", 128, 1024, 32)\n",
        "    activation_function = trial.suggest_categorical(\"activation_function\", [\"relu\", \"elu\"])\n",
        "    droput_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.5)\n",
        "    # Input layers\n",
        "    input = keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "    rescale = keras.layers.Rescaling(1.0 / 255)(input)\n",
        "    conv_in = keras.layers.Conv2D(\n",
        "        filters=64, kernel_size=(7, 7), strides=(2, 2), activation=activation_function\n",
        "    )(rescale)\n",
        "    pool_in = keras.layers.MaxPooling2D(\n",
        "        pool_size=(3, 3), strides=(2, 2), padding=\"same\"\n",
        "    )(conv_in)\n",
        "    # R1\n",
        "    conv_r1_1 = keras.layers.Conv2D(64, 3, 1, padding=\"same\")(pool_in)\n",
        "    bn_r1_1 = keras.layers.BatchNormalization()(conv_r1_1)\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r1_1 = keras.layers.ReLU()(bn_r1_1)\n",
        "    else:\n",
        "        relu_r1_1 = keras.layers.ELU()(bn_r1_1)\n",
        "    conv_r1_2 = keras.layers.Conv2D(64, 3, 1, padding=\"same\")(relu_r1_1)\n",
        "    bn_r1_2 = keras.layers.BatchNormalization()(conv_r1_2)\n",
        "    skip_r1 = keras.layers.Add()([bn_r1_2, pool_in])\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r1_2 = keras.layers.ReLU()(skip_r1)\n",
        "    else:\n",
        "        relu_r1_2 = keras.layers.ELU()(skip_r1)\n",
        "    # R2\n",
        "    conv_r2_1 = keras.layers.Conv2D(64, 3, 1, padding=\"same\")(relu_r1_2)\n",
        "    bn_r2_1 = keras.layers.BatchNormalization()(conv_r2_1)\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r2_1 = keras.layers.ReLU()(bn_r2_1)\n",
        "    else:\n",
        "        relu_r2_1 = keras.layers.ELU()(bn_r2_1)\n",
        "    conv_r2_2 = keras.layers.Conv2D(64, 3, 1, padding=\"same\")(relu_r2_1)\n",
        "    bn_r2_2 = keras.layers.BatchNormalization()(conv_r2_2)\n",
        "    skip_r2 = keras.layers.Add()([bn_r2_2, relu_r1_2])\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r2_2 = keras.layers.ReLU()(skip_r2)\n",
        "    else:\n",
        "        relu_r2_2 = keras.layers.ELU()(skip_r2)\n",
        "    # R3\n",
        "    conv_r3_skip = keras.layers.Conv2D(128, 1, 2, padding=\"same\")(relu_r2_2)\n",
        "    conv_r3_1 = keras.layers.Conv2D(128, 3, 2, padding=\"same\")(relu_r2_2)\n",
        "    bn_r3_1 = keras.layers.BatchNormalization()(conv_r3_1)\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r3_1 = keras.layers.ReLU()(bn_r3_1)\n",
        "    else:\n",
        "        relu_r3_1 = keras.layers.ELU()(bn_r3_1)\n",
        "    conv_r3_2 = keras.layers.Conv2D(128, 3, 1, padding=\"same\")(relu_r3_1)\n",
        "    bn_r3_2 = keras.layers.BatchNormalization()(conv_r3_2)\n",
        "    skip_r3 = keras.layers.Add()([bn_r3_2, conv_r3_skip])\n",
        "    if activation_function == \"relu\":\n",
        "        relu_r3_2 = keras.layers.ReLU()(skip_r3)\n",
        "    else:\n",
        "        relu_r3_2 = keras.layers.ELU()(skip_r3)\n",
        "    # Output layers\n",
        "    pool_out = keras.layers.GlobalAveragePooling2D()(relu_r3_2)\n",
        "    dense_out = keras.layers.Dense(size_dense, activation=activation_function)(pool_out)\n",
        "    dropout = keras.layers.Dropout(droput_rate)\n",
        "    output = keras.layers.Dense(num_classes, activation=\"softmax\")(dense_out)\n",
        "    # Model\n",
        "    model = keras.Model(inputs=input, outputs=output)\n",
        "    # Fitting model\n",
        "    \n",
        "    max_lr = trial.suggest_float(\"max_lr\", 0.01, 0.2)\n",
        "    start_lr_prop = trial.suggest_float(\"start_lr_prop\", 1./50, 1./5)\n",
        "    last_lr = trial.suggest_float(\"last_lr\", 0.0001, 0.01)\n",
        "    lr = trial.suggest_float(\"lr\", 0.001, 0.2)\n",
        "    mom = trial.suggest_float(\"momentum\", 0.8, 0.9)\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        patience=10, restore_best_weights=True, start_from_epoch=20\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.SGD(learning_rate = lr, momentum = mom,\n",
        "                                          nesterov = True),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        train_set,\n",
        "        validation_data=val_set,\n",
        "        epochs=EPOCH,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[early_stopping],\n",
        "    )\n",
        "    # Evaluating and returning F1 score\n",
        "    loss, acc = model.evaluate(val_set)\n",
        "    return acc\n",
        "\n",
        "\n",
        "study = optuna.create_study(\n",
        "    storage=\"sqlite:///drive/MyDrive/maize-crop-diagnose/db_maize_models.sqlite3\",\n",
        "    study_name=\"resnet_drop_sgd\",\n",
        "    direction=\"maximize\",\n",
        "    load_if_exists=True,\n",
        ")\n",
        "\n",
        "study.optimize(objective, n_trials=100)\n",
        "print(f\"Best value: {study.best_value} (params: {study.best_params})\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
